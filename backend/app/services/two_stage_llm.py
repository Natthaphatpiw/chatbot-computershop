import os
import json
import re
from openai import OpenAI
from typing import List, Dict, Any, Tuple
from app.models import Product

# Initialize OpenAI client with error handling
def get_openai_client():
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("OPENAI_API_KEY environment variable is required")
    return OpenAI(api_key=api_key)

client = None

# Enhanced text normalization for better Thai language processing
def normalize_text_advanced(text: str) -> str:
    """Advanced text normalization with comprehensive Thai language support"""
    normalized = text.lower()
    
    # Notebook variations - comprehensive patterns
    notebook_patterns = [
        (r'‡πÇ‡∏ô‡πâ?[‡∏î‡∏ï‡πä]‡∏ö‡∏∏‡πä?[‡∏Ñ‡∏Å]', '‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å'),
        (r'‡πÇ‡∏ô‡∏ï?‡∏ö‡∏∏[‡πä‡∏Ñ]+', '‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å'),
        (r'‡πÇ‡∏ô‡πä‡∏ï‡∏ö‡∏∏[‡πä‡∏Ñ]+', '‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å'),
        (r'‡πÇ‡∏ô‡∏Ñ‡∏ö‡∏∏‡∏Ñ', '‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å'),
        (r'‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡∏Ñ', '‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å'),
        (r'laptop', '‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å'),
        (r'notebook', '‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å')
    ]
    
    # Graphics card variations
    graphics_patterns = [
        (r'‡∏Å‡∏≤‡∏£‡πå[‡∏à‡∏î]‡∏≠', '‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠'),
        (r'[‡∏ß‡∏ü]‡∏µ‡∏à‡∏µ‡πÄ‡∏≠', '‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠'),
        (r'‡∏Å‡∏£‡∏≤‡∏ü‡∏¥?[‡∏Ñ‡∏Å‡∏Ç]', '‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠'),
        (r'vga', '‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠'),
        (r'graphics', '‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠')
    ]
    
    # Computer variations - important for Thai context
    computer_patterns = [
        (r'‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥?‡∏ß‡πÄ?‡∏ï‡∏≠‡∏£‡πå', '‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå'),
        (r'‡∏Ñ‡∏≠‡∏°(?!‡∏û‡∏¥)', '‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå'),  # "‡∏Ñ‡∏≠‡∏°" but not "‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥"
        (r'‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á(?=.*‡∏Ñ‡∏≠‡∏°|.*pc)', '‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå'),
        (r'desktop', '‡∏Ñ‡∏≠‡∏°‡∏ï‡∏±‡πâ‡∏á‡πÇ‡∏ï‡πä‡∏∞'),
        (r'pc', '‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå')
    ]
    
    # Other components
    component_patterns = [
        (r'‡∏Ñ‡∏µ‡∏ö‡∏≠‡∏£‡πå?‡∏î', '‡∏Ñ‡∏µ‡∏¢‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î'),
        (r'‡πÄ‡∏°‡πâ‡∏≤‡∏™‡πå', '‡πÄ‡∏°‡∏≤‡∏™‡πå'),
        (r'‡∏ã‡∏µ‡∏û‡∏µ‡∏¢‡∏π', '‡∏ã‡∏µ‡∏û‡∏µ‡∏¢‡∏π'),
        (r'cpu', '‡∏ã‡∏µ‡∏û‡∏µ‡∏¢‡∏π'),
        (r'‡πÅ‡∏£‡∏°', '‡πÅ‡∏£‡∏°'),
        (r'ram', '‡πÅ‡∏£‡∏°'),
        (r'memory', '‡πÅ‡∏£‡∏°'),
        (r'‡∏´‡∏π‡∏ü‡∏±‡∏á', '‡∏´‡∏π‡∏ü‡∏±‡∏á'),
        (r'headphone', '‡∏´‡∏π‡∏ü‡∏±‡∏á'),
        (r'‡∏à‡∏≠(?!‡∏Ñ‡∏≠‡∏°)', '‡∏°‡∏≠‡∏ô‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå'),
        (r'monitor', '‡∏°‡∏≠‡∏ô‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå'),
        (r'‡πÇ‡∏õ‡∏£‡πÄ‡∏ã‡∏™[‡πÄ‡∏ã‡∏™]‡∏≠‡∏£‡πå', '‡∏ã‡∏µ‡∏û‡∏µ‡∏¢‡∏π')
    ]
    
    # Apply all patterns
    all_patterns = notebook_patterns + graphics_patterns + computer_patterns + component_patterns
    
    for pattern, replacement in all_patterns:
        normalized = re.sub(pattern, replacement, normalized, flags=re.IGNORECASE)
    
    return normalized

# Enhanced contextual phrase segmentation
def contextual_phrase_segmentation(text: str) -> List[str]:
    """
    Segment input text into meaningful phrases based on context
    Example: "‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏Ñ‡∏≠‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡∏¥‡∏Å ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡πà‡∏≠‡∏¢" ‚Üí ["‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏Ñ‡∏≠‡∏°", "‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡∏¥‡∏Å", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡πà‡∏≠‡∏¢"]
    """
    import re
    
    # Normalize first
    normalized = normalize_text_advanced(text)
    
    # Define phrase patterns with context awareness
    phrase_patterns = [
        # Specific product names (highest priority)
        (r'Ryzen\s*\d+\s*\d+\w*', 'PRODUCT_NAME'),     # Ryzen 5 5600G
        (r'Intel\s*Core\s*i\d+', 'PRODUCT_NAME'),       # Intel Core i5
        (r'RTX\s*\d+\w*', 'PRODUCT_NAME'),              # RTX 4060
        (r'GTX\s*\d+\w*', 'PRODUCT_NAME'),              # GTX 1660
        (r'AMD\s*\w*\d+\w*', 'PRODUCT_NAME'),           # AMD 5600G
        
        # Product desire phrases (keep product and desire together)
        (r'‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ(‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å|‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå|‡∏Ñ‡∏≠‡∏°|‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠|‡πÄ‡∏°‡∏≤‡∏™‡πå|‡∏Ñ‡∏µ‡∏¢‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î|‡∏´‡∏π‡∏ü‡∏±‡∏á)', 'PRODUCT_DESIRE'),
        (r'‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£(‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å|‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå|‡∏Ñ‡∏≠‡∏°|‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠|‡πÄ‡∏°‡∏≤‡∏™‡πå|‡∏Ñ‡∏µ‡∏¢‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î|‡∏´‡∏π‡∏ü‡∏±‡∏á)', 'PRODUCT_DESIRE'),
        (r'‡∏´‡∏≤(‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å|‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå|‡∏Ñ‡∏≠‡∏°|‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠|‡πÄ‡∏°‡∏≤‡∏™‡πå|‡∏Ñ‡∏µ‡∏¢‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î|‡∏´‡∏π‡∏ü‡∏±‡∏á)', 'PRODUCT_DESIRE'),
        
        # Product categories as standalone phrases
        (r'‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å', 'PRODUCT'),
        (r'‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå', 'PRODUCT'),
        (r'‡∏Ñ‡∏≠‡∏°(?!‡∏û‡∏¥)', 'PRODUCT'),  # "‡∏Ñ‡∏≠‡∏°" but not "‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥"
        (r'‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠', 'PRODUCT'),
        (r'‡πÄ‡∏°‡∏≤‡∏™‡πå', 'PRODUCT'),
        (r'‡∏Ñ‡∏µ‡∏¢‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î', 'PRODUCT'),
        (r'‡∏´‡∏π‡∏ü‡∏±‡∏á', 'PRODUCT'),
        
        # Usage phrases (more specific patterns)
        (r'‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡∏¥‡∏Å', 'USAGE'),
        (r'‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏®', 'USAGE'),
        (r'‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£', 'USAGE'),
        (r'‡∏ó‡∏≥‡∏á‡∏≤‡∏ô(?!‡∏Å‡∏£‡∏≤‡∏ü‡∏¥‡∏Å|‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏®)', 'USAGE'),  # General work
        (r'‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°', 'QUESTION'),         # Specific gaming question - higher priority
        (r'‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°\s*[\w\s]*', 'USAGE'),
        (r'‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô[\w\s]*', 'USAGE'),
        (r'‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö[\w\s]*(?=\s|$)', 'USAGE'),
        
        # Budget phrases
        (r'‡∏á‡∏ö\s*\d+(?:,\d+)*', 'BUDGET'),
        (r'‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô\s*\d+(?:,\d+)*', 'BUDGET'),
        (r'‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì\s*\d+(?:,\d+)*', 'BUDGET'),
        (r'‡∏£‡∏≤‡∏Ñ‡∏≤[\s\w]*\d+(?:,\d+)*', 'BUDGET'),
        
        # Brand phrases
        (r'(ASUS|HP|Dell|MSI|Acer|Lenovo|Apple|Razer|Logitech|Corsair)', 'BRAND'),
        (r'‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠\s*\w+', 'BRAND'),
        
        # Request phrases (NON-FILTER - should be skipped in Stage 1)
        (r'‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡πà‡∏≠‡∏¢', 'REQUEST'),
        (r'‡∏£‡∏∏‡πà‡∏ô‡πÑ‡∏´‡∏ô‡∏î‡∏µ', 'REQUEST'),
        (r'‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á', 'REQUEST'),
        (r'‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥\w*', 'REQUEST'),
        
        # Question phrases (NON-FILTER - should be skipped in Stage 1)
        (r'‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°', 'QUESTION'),       # ‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°
        (r'\w+‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°', 'QUESTION'),          # ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°, ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°
        (r'‡∏î‡∏µ‡πÑ‡∏´‡∏°', 'QUESTION'),              # ‡∏î‡∏µ‡πÑ‡∏´‡∏°
        (r'\w+‡∏î‡∏µ‡πÑ‡∏´‡∏°', 'QUESTION'),           # ‡πÉ‡∏ä‡πâ‡∏î‡∏µ‡πÑ‡∏´‡∏°
        (r'‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£', 'QUESTION'),         # ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
        
        # Spec phrases
        (r'RTX\s*\d+', 'SPEC'),
        (r'GTX\s*\d+', 'SPEC'),
        (r'\d+GB\s*(RAM|‡πÅ‡∏£‡∏°)', 'SPEC'),
        (r'mechanical', 'SPEC'),
        (r'‡πÑ‡∏£‡πâ‡∏™‡∏≤‡∏¢', 'SPEC'),
        (r'RGB', 'SPEC'),
    ]
    
    phrases = []
    processed_text = normalized.lower()
    
    # Extract phrases using patterns
    for pattern, phrase_type in phrase_patterns:
        matches = re.finditer(pattern, processed_text, re.IGNORECASE)
        for match in matches:
            phrase = match.group().strip()
            if phrase and phrase not in phrases:
                phrases.append(phrase)
                # Remove matched phrase to avoid overlap
                processed_text = processed_text.replace(phrase.lower(), ' ', 1)
    
    # Handle remaining single words that might be important
    remaining_words = re.findall(r'\b\w{2,}\b', processed_text)
    category_keywords = ['‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å', '‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå', '‡∏Ñ‡∏≠‡∏°', '‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠', '‡πÄ‡∏°‡∏≤‡∏™‡πå', '‡∏Ñ‡∏µ‡∏¢‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î', '‡∏´‡∏π‡∏ü‡∏±‡∏á']
    
    for word in remaining_words:
        if any(keyword in word for keyword in category_keywords) and word not in phrases:
            phrases.append(word)
    
    # If no phrases found, fallback to simple splitting
    if not phrases:
        phrases = [text]
    
    return phrases

# Load database schema and categories
def load_database_schema():
    """Load actual database schema and categories"""
    schema_data = {}
    categories_data = []
    
    # Load schema.json
    try:
        schema_paths = ['../../../schema.json', '../../schema.json', '../schema.json', 'schema.json']
        
        for path in schema_paths:
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    schema_data = json.load(f)
                    print(f"‚úÖ Loaded schema from {path}")
                    break
            except FileNotFoundError:
                continue
    except Exception as e:
        print(f"Warning: Could not load schema.json: {e}")
    
    # Load navigation_attributes.json
    try:
        nav_paths = ['../../../navigation_attributes.json', '../../navigation_attributes.json', '../navigation_attributes.json', 'navigation_attributes.json']
        
        for path in nav_paths:
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    nav_data = json.load(f)
                    categories_data = nav_data.get('cateName', [])
                    print(f"‚úÖ Loaded {len(categories_data)} categories from {path}")
                    break
            except FileNotFoundError:
                continue
    except Exception as e:
        print(f"Warning: Could not load navigation_attributes.json: {e}")
    
    return schema_data, categories_data

# Comprehensive Thai-English category mapping
def get_comprehensive_category_mapping() -> Dict[str, List[str]]:
    """Enhanced category mapping with comprehensive Thai terms"""
    return {
        # Notebook/Laptop categories
        '‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å': ['Notebooks', 'Gaming Notebooks', 'Ultrathin Notebooks', '2 in 1 Notebooks'],
        '‡πÇ‡∏ô‡∏ï‡∏ö‡∏∏‡πä‡∏Å': ['Notebooks', 'Gaming Notebooks', 'Ultrathin Notebooks', '2 in 1 Notebooks'],  
        '‡πÇ‡∏ô‡πä‡∏ï‡∏ö‡∏∏‡πä‡∏Ñ': ['Notebooks', 'Gaming Notebooks', 'Ultrathin Notebooks', '2 in 1 Notebooks'],
        '‡πÇ‡∏ô‡∏Ñ‡∏ö‡∏∏‡∏Ñ': ['Notebooks', 'Gaming Notebooks', 'Ultrathin Notebooks', '2 in 1 Notebooks'],
        'laptop': ['Notebooks', 'Gaming Notebooks', 'Ultrathin Notebooks', '2 in 1 Notebooks'],
        
        # Computer categories - crucial for Thai context
        '‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå': ['Desktop PC', 'All in One PC (AIO)', 'Mini PC', 'Notebooks'],
        '‡∏Ñ‡∏≠‡∏°': ['Desktop PC', 'All in One PC (AIO)', 'Mini PC', 'Notebooks'],
        '‡∏Ñ‡∏≠‡∏°‡∏ï‡∏±‡πâ‡∏á‡πÇ‡∏ï‡πä‡∏∞': ['Desktop PC', 'All in One PC (AIO)', 'Mini PC'],
        '‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á': ['Desktop PC', 'All in One PC (AIO)', 'Mini PC', 'Notebooks'],
        'desktop': ['Desktop PC', 'All in One PC (AIO)', 'Mini PC'],
        'pc': ['Desktop PC', 'All in One PC (AIO)', 'Mini PC', 'Notebooks'],
        
        # Graphics cards
        '‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠': ['Graphics Cards'],
        '‡∏Å‡∏≤‡∏£‡πå‡∏à‡∏≠': ['Graphics Cards'],
        '‡∏Å‡∏£‡∏≤‡∏ü‡∏¥‡∏Å': ['Graphics Cards'], 
        '‡∏Å‡∏≤‡∏£‡πå‡∏î': ['Graphics Cards'],
        '‡∏ß‡∏µ‡∏à‡∏µ‡πÄ‡∏≠': ['Graphics Cards'],
        'vga': ['Graphics Cards'],
        'graphics': ['Graphics Cards'],
        
        # Input devices
        '‡∏Ñ‡∏µ‡∏¢‡πå‡∏ö‡∏≠‡∏£‡πå‡∏î': ['Keyboard', 'Mechanical & Gaming Keyboard', 'Wireless Keyboard'],
        '‡∏Ñ‡∏µ‡∏ö‡∏≠‡∏£‡πå‡∏î': ['Keyboard', 'Mechanical & Gaming Keyboard', 'Wireless Keyboard'],
        '‡∏Ñ‡∏µ‡∏ö‡∏≠‡∏î': ['Keyboard', 'Mechanical & Gaming Keyboard', 'Wireless Keyboard'],
        'keyboard': ['Keyboard', 'Mechanical & Gaming Keyboard', 'Wireless Keyboard'],
        
        '‡πÄ‡∏°‡∏≤‡∏™‡πå': ['Mouse', 'Gaming Mouse', 'Wireless Mouse'],
        '‡πÄ‡∏°‡πâ‡∏≤‡∏™‡πå': ['Mouse', 'Gaming Mouse', 'Wireless Mouse'],
        'mouse': ['Mouse', 'Gaming Mouse', 'Wireless Mouse'],
        
        # Display
        '‡∏à‡∏≠': ['Monitor'],
        '‡∏°‡∏≠‡∏ô‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå': ['Monitor'],
        '‡∏à‡∏≠‡∏°‡∏≠‡∏ô‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå': ['Monitor'],
        '‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠': ['Monitor'],
        'monitor': ['Monitor'],
        
        # Components
        '‡∏ã‡∏µ‡∏û‡∏µ‡∏¢‡∏π': ['CPU'],
        '‡πÇ‡∏õ‡∏£‡πÄ‡∏ã‡∏™‡πÄ‡∏ã‡∏≠‡∏£‡πå': ['CPU'],
        'cpu': ['CPU'],
        'processor': ['CPU'],
        
        '‡πÅ‡∏£‡∏°': ['RAM'],
        '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥': ['RAM'],
        '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥': ['RAM'],
        'ram': ['RAM'],
        'memory': ['RAM'],
        
        # Audio
        '‡∏´‡∏π‡∏ü‡∏±‡∏á': ['Headphone', 'Headset', 'In Ear Headphone', 'True Wireless Headphone'],
        '‡πÄ‡∏Æ‡∏î‡πÇ‡∏ü‡∏ô': ['Headphone', 'Headset', 'In Ear Headphone', 'True Wireless Headphone'],
        '‡πÄ‡∏Æ‡∏î‡πÄ‡∏ã‡∏ï': ['Headphone', 'Headset', 'In Ear Headphone', 'True Wireless Headphone'],
        'headphone': ['Headphone', 'Headset', 'In Ear Headphone', 'True Wireless Headphone'],
        'headset': ['Headphone', 'Headset', 'In Ear Headphone', 'True Wireless Headphone'],
        
        '‡∏™‡∏õ‡∏µ‡∏Å‡πÄ‡∏Å‡∏≠‡∏£‡πå': ['Speaker'],
        'speaker': ['Speaker'],
        
        # Storage
        '‡∏Æ‡∏≤‡∏£‡πå‡∏î‡∏î‡∏¥‡∏™‡∏Å‡πå': ['Hard Drive & Solid State Drive'],
        '‡∏Æ‡∏≤‡∏£‡πå‡∏î': ['Hard Drive & Solid State Drive'],
        '‡πÄ‡∏≠‡∏™‡πÄ‡∏≠‡∏™‡∏î‡∏µ': ['Hard Drive & Solid State Drive'],
        'ssd': ['Hard Drive & Solid State Drive'],
        'hdd': ['Hard Drive & Solid State Drive']
    }

# LLM Stage 1: Basic Query Builder - NO content analysis
async def stage1_basic_query_builder(user_input: str) -> Dict[str, Any]:
    """
    Stage 1 LLM: Build basic MongoDB query for initial filtering only
    Focuses ONLY on: category, price, stock - NO title/description analysis
    """
    normalized_input = normalize_text_advanced(user_input)
    
    # Load database information
    schema_data, categories_data = load_database_schema()
    category_mapping = get_comprehensive_category_mapping()
    
    # Get actual database fields
    actual_fields = []
    if schema_data and 'properties' in schema_data:
        actual_fields = list(schema_data['properties'].keys())
    
    # Convert data to strings to avoid f-string issues
    fields_str = str(actual_fields)
    categories_str = json.dumps(categories_data, ensure_ascii=False)
    mapping_str = json.dumps(category_mapping, ensure_ascii=False, indent=2)
    
    prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Stage 1 Query Builder - ‡∏°‡∏µ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á MongoDB Query ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

**USER INPUT:** "{user_input}"
**NORMALIZED:** "{normalized_input}"

**DATABASE FIELDS:** {fields_str}
**AVAILABLE CATEGORIES:** {categories_str}
**CATEGORY MAPPING:** {mapping_str}

**STAGE 1 RESPONSIBILITIES (‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ô‡∏µ‡πâ):**
1. **CONTEXTUAL INPUT ANALYSIS** - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥
2. **SEMANTIC PHRASE SEGMENTATION** - ‡πÅ‡∏¢‡∏Å‡∏ß‡∏•‡∏µ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß
3. **BASIC CATEGORY IDENTIFICATION** - ‡∏´‡∏≤‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏´‡∏•‡∏±‡∏Å‡∏à‡∏≤‡∏Å input ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏ô‡∏∏‡∏°‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
4. **BUDGET EXTRACTION** - ‡∏´‡∏≤‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏∏  
5. **BASIC FIELD MAPPING** - ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô MongoDB query ‡∏á‡πà‡∏≤‡∏¢‡πÜ
6. **NO CONTENT ANALYSIS** - ‡∏´‡πâ‡∏≤‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå title/description/brand/specs
7. **SMART CATEGORY INFERENCE** - ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÉ‡∏´‡πâ‡∏≠‡∏ô‡∏∏‡∏°‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó

**STRICT RULES - STAGE 1:**
- ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞: stockQuantity, cateName, salePrice
- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ: title, description, $regex, $or ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö content
- ‡∏´‡πâ‡∏≤‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå: ‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå, ‡∏™‡πÄ‡∏õ‡∏Ñ, ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞, ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏°
- **CONTEXTUAL UNDERSTANDING**: ‡∏≠‡πà‡∏≤‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à = ‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà‡πÉ‡∏ô query (‡πÉ‡∏´‡πâ Stage 2 ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£)
- **CATEGORY INFERENCE**: ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÉ‡∏´‡πâ‡∏≠‡∏ô‡∏∏‡∏°‡∏≤‡∏ô‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
- **NON-FILTER PHRASES**: ‡∏Ñ‡∏≥‡∏Ç‡∏≠/‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡πÄ‡∏ä‡πà‡∏ô "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡πà‡∏≠‡∏¢", "‡∏£‡∏∏‡πà‡∏ô‡πÑ‡∏´‡∏ô‡∏î‡∏µ", "‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°" ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà filter - ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ

**CONTEXTUAL ANALYSIS EXAMPLES:**

Input: "‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Ñ ASUS ‡∏á‡∏ö 20000"
CONTEXT ANALYSIS:
- "‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å" = ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‚Üí cateName: "Notebooks" ‚úÖ (can filter)
- "ASUS" = ‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå ‚Üí ‚ùå (cannot filter in Stage 1 - leave for Stage 2)  
- "‡∏á‡∏ö 20000" = ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì ‚Üí salePrice: max 20000 ‚úÖ (can filter)

Input: "‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏Ñ‡∏≠‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡∏¥‡∏Å ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡πà‡∏≠‡∏¢"
CONTEXT ANALYSIS:
- "‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏Ñ‡∏≠‡∏°" = ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå ‚Üí cateName: ["Desktop PC", "Notebooks"] ‚úÖ (can filter)
- "‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡∏¥‡∏Å" = ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‚Üí ‚ùå (usage analysis - leave for Stage 2)
- "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡πà‡∏≠‡∏¢" = ‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ ‚Üí ‚ùå (NON-FILTER request phrase - skip completely)

Input: "‡∏Ñ‡∏≠‡∏°‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏° valorant ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 30000"
CONTEXT ANALYSIS:
- "‡∏Ñ‡∏≠‡∏°" = ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ ‚Üí cateName: ["Desktop PC", "Gaming Notebooks"] ‚úÖ (can filter)
- "‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏° valorant" = ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‚Üí ‚ùå (usage + game analysis - leave for Stage 2)
- "‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 30000" = ‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì ‚Üí salePrice: max 30000 ‚úÖ (can filter)

Input: "Ryzen 5 5600G ‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°"
CONTEXT ANALYSIS:
- "Ryzen 5 5600G" = ‡∏ä‡∏∑‡πà‡∏≠ CPU ‡∏£‡∏∏‡πà‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‚Üí ‚ùå (specific product name - leave for Stage 2)
- "‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°" = ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‚Üí ‚ùå (usage question - leave for Stage 2)
- ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‚Üí ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ Stage 1 ‡∏≠‡∏ô‡∏∏‡∏°‡∏≤‡∏ô CPU, Notebooks ‚Üí cateName: ["CPU", "Notebooks"] ‚úÖ (inferred category)

**BUDGET PATTERNS:**
- "‡∏á‡∏ö X", "‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô X", "‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì X", "‡∏£‡∏≤‡∏Ñ‡∏≤ X ‡∏ö‡∏≤‡∏ó" ‚Üí max budget
- "X-Y ‡∏ö‡∏≤‡∏ó", "‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á X ‡∏ñ‡∏∂‡∏á Y" ‚Üí price range

**PHRASE SEGMENTATION RULES:**

**‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏ß‡∏•‡∏µ‡∏ï‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß):**
1. **Product Category Phrases**: "‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏Ñ‡∏≠‡∏°", "‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å", "‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠"
2. **Usage Phrases**: "‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡∏¥‡∏Å", "‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏° valorant", "‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏≠‡∏≠‡∏ü‡∏ü‡∏¥‡∏®"
3. **Budget Phrases**: "‡∏á‡∏ö 15000", "‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 20000", "‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 30000"
4. **Brand Phrases**: "‡∏¢‡∏µ‡πà‡∏´‡πâ‡∏≠ ASUS", "HP ‡∏´‡∏£‡∏∑‡∏≠ Dell", "MSI gaming"
5. **Request Phrases**: "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡πà‡∏≠‡∏¢", "‡∏£‡∏∏‡πà‡∏ô‡πÑ‡∏´‡∏ô‡∏î‡∏µ", "‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á"

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏ß‡∏•‡∏µ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á:**
- "‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏Ñ‡∏≠‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡∏¥‡∏Å ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡πà‡∏≠‡∏¢" 
  ‚Üí ["‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡∏Ñ‡∏≠‡∏°", "‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡∏¥‡∏Å", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡πà‡∏≠‡∏¢"]
- "‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Ñ ASUS ‡∏á‡∏ö 20000"
  ‚Üí ["‡πÇ‡∏ô‡πâ‡∏ï‡∏ö‡∏∏‡πä‡∏Å", "ASUS", "‡∏á‡∏ö 20000"]
- "‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏° RTX 4060"
  ‚Üí ["‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠", "‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°", "RTX 4060"]

**THAI COMPUTER CONTEXT:**
- "‡∏Ñ‡∏≠‡∏°" = ‡∏≠‡∏≤‡∏à‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á Desktop PC, All in One PC, Mini PC, ‡∏´‡∏£‡∏∑‡∏≠‡πÅ‡∏°‡πâ‡πÅ‡∏ï‡πà Notebooks
- "‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á" = ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ

**CRITICAL MONGODB SYNTAX:**
- **Single category**: "cateName": "Notebooks"
- **Multiple categories**: "cateName": {{"$in": ["Desktop PC", "Notebooks"]}}
- **NEVER**: "cateName": ["Desktop PC", "Notebooks"] (‡∏ú‡∏¥‡∏î!)
- **ALWAYS use $in for arrays!**

‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô JSON:
{{
  "mongoQuery": {{
    "stockQuantity": {{"$gt": 0}}
    // ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞ cateName ‡πÅ‡∏•‡∏∞ salePrice ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ
  }},
  "processedTerms": {{
    "category": "‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ",
    "budget": {{"max": number}},
    "segmentedPhrases": ["‡∏ß‡∏•‡∏µ‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡∏ö‡∏£‡∏¥‡∏ö‡∏ó"],
    "used": ["‡∏ß‡∏•‡∏µ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô query ‡πÅ‡∏•‡πâ‡∏ß"],
    "remaining": ["‡∏ß‡∏•‡∏µ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÉ‡∏´‡πâ Stage 2 ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"],
    "analysis": "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏ß‡∏•‡∏µ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•"
  }},
  "reasoning": "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡πÑ‡∏´‡∏ô‡∏ó‡∏≥ query ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡πÑ‡∏´‡∏ô‡∏õ‡∏•‡πà‡∏≠‡∏¢‡πÉ‡∏´‡πâ Stage 2",
  "queryType": "basic_filter"
}}

**‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:** Stage 1 ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏£‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô - ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡πà‡∏á!
"""

    try:
        global client
        if client is None:
            client = get_openai_client()
            
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
        )
        
        content = response.choices[0].message.content.strip()
        
        # Clean JSON response
        if content.startswith('```json'):
            content = content.replace('```json', '').replace('```', '').strip()
        elif content.startswith('```'):
            content = content.replace('```', '').strip()
        
        result = json.loads(content)
        
        # Validate query structure
        query = result["mongoQuery"]
        validated_query = validate_stage1_query(query, actual_fields)
        
        return {
            "query": validated_query,
            "processedTerms": result["processedTerms"],
            "reasoning": result.get("reasoning", "Stage 1 basic filtering"),
            "queryType": result.get("queryType", "basic_filter"),
            "confidence": 0.8
        }
        
    except Exception as error:
        print(f"Stage 1 query generation error: {error}")
        return generate_stage1_fallback(normalized_input, categories_data)

def validate_stage1_query(query: Dict[str, Any], actual_fields: List[str]) -> Dict[str, Any]:
    """Validate Stage 1 query - only allow basic fields"""
    validated_query = {"stockQuantity": {"$gt": 0}}
    
    # Only allow these fields in Stage 1
    allowed_fields = ['stockQuantity', 'cateName', 'salePrice', 'cateId', 'categoryId']
    
    for key, value in query.items():
        if key in allowed_fields:
            validated_query[key] = value
    
    return validated_query

def generate_stage1_fallback(input_text: str, categories_data: List[str]) -> Dict[str, Any]:
    """Fallback for Stage 1 when LLM fails"""
    processed_terms = extract_basic_entities(input_text, categories_data)
    query = build_basic_query(processed_terms)
    
    return {
        "query": query,
        "processedTerms": processed_terms,
        "reasoning": "Stage 1 fallback - basic pattern matching",
        "queryType": "basic_filter",
        "confidence": 0.6
    }

def extract_basic_entities(input_text: str, categories_data: List[str]) -> Dict[str, Any]:
    """Extract basic entities for Stage 1 fallback with improved phrase segmentation"""
    normalized_input = normalize_text_advanced(input_text.lower())
    category_mapping = get_comprehensive_category_mapping()
    
    # Improved contextual phrase segmentation
    segmented_phrases = contextual_phrase_segmentation(input_text)
    
    processed_terms = {
        "segmentedPhrases": segmented_phrases,
        "used": [],
        "remaining": segmented_phrases.copy(),  # Start with all phrases, filter as we process
        "analysis": "Fallback segmentation using pattern matching"
    }
    
    # Find category from segmented phrases
    found_categories = []
    inferred_categories = []
    
    for phrase in segmented_phrases:
        phrase_lower = phrase.lower()
        
        # Skip non-filter phrases (requests and questions)
        if is_non_filter_phrase(phrase):
            continue  # Skip completely - don't use for filtering
        
        # Direct category matching first
        for thai_term, english_categories in category_mapping.items():
            if thai_term in phrase_lower:
                # Collect all matching categories
                matching_cats = [cat for cat in english_categories if cat in categories_data]
                if matching_cats:
                    found_categories.extend(matching_cats)
                    processed_terms["used"].append(phrase)
                    # Remove from remaining
                    if phrase in processed_terms["remaining"]:
                        processed_terms["remaining"].remove(phrase)
                    break
        else:
            # If no direct category match, try to infer from product names
            if is_specific_product_name(phrase):
                inferred_cats = infer_categories_from_product_name(phrase, categories_data)
                if inferred_cats:
                    inferred_categories.extend(inferred_cats)
                    if phrase not in processed_terms["used"]:
                        processed_terms["used"].append(phrase)  # Mark as used for inference
                    # Keep in remaining for Stage 2 analysis - specific product names need Stage 2
    
    # Set category - prioritize direct matches, then inferred
    all_categories = found_categories + inferred_categories
    if all_categories:
        unique_categories = list(set(all_categories))
        if len(unique_categories) == 1:
            processed_terms["category"] = unique_categories[0]
        else:
            processed_terms["categories"] = unique_categories  # Multiple categories
    
    # Extract budget from segmented phrases  
    budget_patterns = [
        r'‡∏á‡∏ö\s*(\d+(?:,\d{3})*)',
        r'‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô\s*(\d+(?:,\d{3})*)',
        r'‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì\s*(\d+(?:,\d{3})*)',
        r'(\d+(?:,\d{3})*)\s*‡∏ö‡∏≤‡∏ó'
    ]
    
    for phrase in processed_terms["remaining"].copy():  # Use copy to avoid modification during iteration
        for pattern in budget_patterns:
            matches = re.findall(pattern, phrase.lower())
            if matches:
                budget = int(matches[0].replace(',', ''))
                if budget >= 1000:
                    processed_terms["budget"] = {"max": budget}
                    if phrase not in processed_terms["used"]:
                        processed_terms["used"].append(phrase)
                    # Remove from remaining - budget phrases don't need Stage 2
                    if phrase in processed_terms["remaining"]:
                        processed_terms["remaining"].remove(phrase)
                    break
    
    return processed_terms

def is_non_filter_phrase(phrase: str) -> bool:
    """Check if phrase is a non-filter phrase (request/question) that shouldn't be used for filtering"""
    phrase_lower = phrase.lower()
    
    # Request patterns
    request_patterns = [
        r'‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥', r'‡∏£‡∏∏‡πà‡∏ô‡πÑ‡∏´‡∏ô‡∏î‡∏µ', r'‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á', r'‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏´‡∏ô‡πà‡∏≠‡∏¢'
    ]
    
    # Question patterns  
    question_patterns = [
        r'\w+‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°',      # ‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°, ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°
        r'\w+‡∏î‡∏µ‡πÑ‡∏´‡∏°',       # ‡∏î‡∏µ‡πÑ‡∏´‡∏°
        r'‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£'      # ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
    ]
    
    all_patterns = request_patterns + question_patterns
    
    for pattern in all_patterns:
        if re.search(pattern, phrase_lower):
            return True
    
    return False

def is_specific_product_name(phrase: str) -> bool:
    """Check if phrase looks like a specific product name"""
    phrase_lower = phrase.lower()
    
    # CPU patterns
    cpu_patterns = [
        r'ryzen\s*\d+\s*\d+\w*',  # Ryzen 5 5600G, Ryzen 7 5800X
        r'intel?\s*core?\s*i\d+',  # Intel Core i5, i7
        r'intel?\s*\w*\d+\w*',     # Intel 12700K
        r'amd\s*\w*\d+\w*',        # AMD 5600G
    ]
    
    # GPU patterns  
    gpu_patterns = [
        r'(rtx|gtx)\s*\d+\w*',     # RTX 4060, GTX 1660
        r'(radeon|rx)\s*\d+\w*',   # RX 6600 XT
    ]
    
    # Product model patterns
    model_patterns = [
        r'[a-z]+\s*\d+\w*',        # Generic model patterns
        r'\w+\s+[a-z]\d+\w*',      # Brand + model number
    ]
    
    all_patterns = cpu_patterns + gpu_patterns + model_patterns
    
    for pattern in all_patterns:
        if re.search(pattern, phrase_lower, re.IGNORECASE):
            return True
    
    return False

def infer_categories_from_product_name(phrase: str, categories_data: List[str]) -> List[str]:
    """Infer categories from specific product names"""
    phrase_lower = phrase.lower()
    inferred_categories = []
    
    # CPU inference
    cpu_patterns = [
        r'ryzen', r'intel', r'amd', r'cpu', r'processor', r'i\d+', r'\d+\w*g$'
    ]
    
    if any(re.search(pattern, phrase_lower) for pattern in cpu_patterns):
        # CPU could be standalone or in notebooks
        possible_cats = ['CPU', 'Notebooks', 'Desktop PC']
        inferred_categories.extend([cat for cat in possible_cats if cat in categories_data])
    
    # GPU inference
    gpu_patterns = [
        r'rtx', r'gtx', r'radeon', r'rx', r'graphics'
    ]
    
    if any(re.search(pattern, phrase_lower) for pattern in gpu_patterns):
        possible_cats = ['Graphics Cards', 'Gaming Notebooks', 'Desktop PC']
        inferred_categories.extend([cat for cat in possible_cats if cat in categories_data])
    
    return list(set(inferred_categories))

def build_basic_query(processed_terms: Dict[str, Any]) -> Dict[str, Any]:
    """Build basic MongoDB query from processed terms"""
    query = {"stockQuantity": {"$gt": 0}}
    
    # Add category - handle both single and multiple categories
    if processed_terms.get("category"):
        # Single category
        query["cateName"] = processed_terms["category"]
    elif processed_terms.get("categories"):
        # Multiple categories - use $in operator
        query["cateName"] = {"$in": processed_terms["categories"]}
    
    # Add budget if found
    if processed_terms.get("budget", {}).get("max"):
        query["salePrice"] = {"$lte": processed_terms["budget"]["max"]}
    
    return query

# LLM Stage 2: Deep Content Analyzer and Product Matcher
async def stage2_content_analyzer(
    user_input: str,
    stage1_result: Dict[str, Any],
    products: List[Product]
) -> List[Product]:
    """
    Stage 2 LLM: Deep content analysis and product matching
    Analyzes remaining terms from Stage 1 and matches against title/description
    """
    if len(products) == 0:
        return []
    
    processed_terms = stage1_result.get("processedTerms", {})
    remaining_terms = processed_terms.get("remaining", [user_input])
    used_terms = processed_terms.get("used", [])
    
    # If no remaining terms to analyze, just sort by popularity
    if not remaining_terms or (len(remaining_terms) == 1 and remaining_terms[0] == user_input and not used_terms):
        print("[Stage 2] No specific analysis needed - using popularity sorting")
        return sorted(products, 
                     key=lambda p: (p.productView, p.rating, -p.salePrice), 
                     reverse=True)[:8]
    
    print(f"[Stage 2] Analyzing {len(products)} products for remaining terms: {remaining_terms}")
    
    # Prepare products info for LLM analysis
    products_info = ""
    for i, p in enumerate(products[:15]):  # Limit to 15 products for token efficiency
        discount = p.price - p.salePrice
        discount_text = ""
        if discount > 0:
            discount_percent = round((discount / p.price) * 100)
            discount_text = f" (‡∏•‡∏î {discount_percent}%)"
        
        price_formatted = f"‡∏ø{p.salePrice:,}"
        views_formatted = f"{p.productView:,}"
        
        products_info += f"""
Product {i + 1}:
Title: {p.title}
Description: {p.description[:200]}...
Category: {p.cateName}
Price: {price_formatted}{discount_text}
Rating: {p.rating}/5 ({p.totalReviews})
Views: {views_formatted}
Stock: {p.stockQuantity}
"""
    
    prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Stage 2 Content Analyzer - ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

**ORIGINAL USER INPUT:** "{user_input}"

**STAGE 1 PROCESSING RESULTS:**
- **Segmented Phrases:** {processed_terms.get('segmentedPhrases', [])}
- **Used Phrases (Stage 1 ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡πâ‡∏ß):** {used_terms}
- **Remaining Phrases (‡πÉ‡∏´‡πâ Stage 2 ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå):** {remaining_terms}
- **Segmentation Analysis:** {processed_terms.get('analysis', 'N/A')}
- **MongoDB Query Applied:** {json.dumps(stage1_result.get('query', {}), ensure_ascii=False)}
- **Stage 1 Reasoning:** {stage1_result.get('reasoning', 'N/A')}

**PRODUCTS TO ANALYZE:**
{products_info}

**STAGE 2 ANALYSIS MISSION:**
1. **‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Remaining Terms** - ‡πÉ‡∏ô‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà Stage 1 ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£
2. **‡∏£‡∏∞‡∏ö‡∏∏‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•** - ‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå (title), ‡∏™‡πÄ‡∏õ‡∏Ñ/‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (description), ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥ (title+description)
3. **‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤** - ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏ü‡∏¥‡∏•‡∏î‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (title ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå, description ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô)
4. **‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°** - 0-100 ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö remaining terms
5. **SPECIFIC PRODUCT MATCHING** - ‡∏ñ‡πâ‡∏≤‡∏ß‡∏•‡∏µ‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÉ‡∏´‡πâ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö

**ANALYSIS GUIDELINES:**

**‡∏ä‡∏∑‡πà‡∏≠‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞** ‚Üí ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô **TITLE** (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î):
- "Ryzen 5 5600G", "Intel Core i5", "RTX 4060", "GTX 1660"
- "MacBook Pro", "ThinkPad", "Pavilion", "Inspiron"
- "MX Master", "K95 RGB", "Razer DeathAdder"
- **‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£**: ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠ ‡πÄ‡∏ä‡πà‡∏ô "5600G" ‡∏≠‡∏≤‡∏à‡∏û‡∏ö‡πÉ‡∏ô "AMD RYZEN 5 5600G 3.9 GHz"

**‡πÅ‡∏ö‡∏£‡∏ô‡∏î‡πå/‡∏£‡∏∏‡πà‡∏ô** ‚Üí ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô **TITLE**:
- "ASUS", "HP", "Dell", "MSI", "Acer", "Lenovo", "Apple"
- "AMD", "Intel", "NVIDIA" 
- "Razer", "Logitech", "Corsair"

**‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô/‡πÄ‡∏Å‡∏°** ‚Üí ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô **DESCRIPTION**:
- "‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°", "gaming", "valorant", "pubg", "gta", "cyberpunk"
- "‡∏ó‡∏≥‡∏á‡∏≤‡∏ô", "office", "excel", "photoshop", "video editing"
- "‡πÄ‡∏£‡∏µ‡∏¢‡∏ô", "streaming", "design", "programming"

**‡∏™‡πÄ‡∏õ‡∏Ñ‡πÄ‡∏â‡∏û‡∏≤‡∏∞** ‚Üí ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô **TITLE + DESCRIPTION**:
- "16GB RAM", "512GB SSD", "144Hz", "4K", "RGB"
- "mechanical", "wireless", "bluetooth", "USB-C"
- "RTX", "GTX", "Intel", "AMD", "Core i7"

**SCORING CRITERIA:**
- **Perfect Match** (90-100): ‡∏ï‡∏£‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≥‡πÉ‡∏ô remaining terms
- **Good Match** (70-89): ‡∏ï‡∏£‡∏á‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏Ç‡∏≠‡∏á remaining terms  
- **Partial Match** (50-69): ‡∏ï‡∏£‡∏á‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á remaining terms
- **Weak Match** (30-49): ‡∏ï‡∏£‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô
- **No Match** (0-29): ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏•‡∏¢

**EXAMPLES:**

Remaining: ["ASUS"] ‚Üí ‡∏î‡∏π title ‡∏ß‡πà‡∏≤‡∏°‡∏µ "ASUS" ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
Remaining: ["valorant"] ‚Üí ‡∏î‡∏π description ‡∏ß‡πà‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏™‡πÄ‡∏õ‡∏Ñ‡πÄ‡∏•‡πà‡∏ô valorant ‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà  
Remaining: ["RGB", "mechanical"] ‚Üí ‡∏î‡∏π title+description ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
Remaining: ["Ryzen 5 5600G"] ‚Üí ‡∏î‡∏π title ‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "5600G" ‡∏´‡∏£‡∏∑‡∏≠ "RYZEN 5 5600G" ‡πÉ‡∏ô title
Remaining: ["RTX 4060"] ‚Üí ‡∏î‡∏π title ‡∏ß‡πà‡∏≤‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡πÑ‡∏´‡∏ô‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "RTX 4060" ‡πÉ‡∏ô title

**SPECIFIC PRODUCT NAME MATCHING STRATEGY:**
1. **‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**: "Ryzen 5 5600G" ‚Üí ["Ryzen", "5600G", "AMD"]
2. **‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Title**: ‡∏´‡∏≤‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÉ‡∏ô title
3. **‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏ô Title**: ‡∏•‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô description 
4. **‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á**: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡πÉ‡∏ô title = 95-100 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô, ‡πÉ‡∏ô description = 70-80 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô

‡∏ï‡∏≠‡∏ö‡πÉ‡∏ô JSON:
{{
  "selectedProducts": [
    {{
      "index": 0,
      "score": 95,
      "matchDetails": {{
        "titleMatches": ["‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô title"],
        "descriptionMatches": ["‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô description"],  
        "reasoning": "‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ô‡∏µ‡πâ"
      }}
    }}
  ],
  "analysisSummary": "‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå remaining terms",
  "unmatchedTerms": ["‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ô‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÉ‡∏î‡πÄ‡∏•‡∏¢"]
}}

**‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:** ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏â‡∏û‡∏≤‡∏∞ remaining terms ‡∏ó‡∏µ‡πà Stage 1 ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£!
"""

    try:
        global client
        if client is None:
            client = get_openai_client()
            
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        
        content = response.choices[0].message.content.strip()
        
        # Clean JSON response
        if content.startswith('```json'):
            content = content.replace('```json', '').replace('```', '').strip()
        elif content.startswith('```'):
            content = content.replace('```', '').strip()
        
        result = json.loads(content)
        selected_products_data = result.get("selectedProducts", [])
        
        # Build filtered and ranked products list
        filtered_products = []
        for item in selected_products_data:
            idx = item.get("index", -1)
            if 0 <= idx < len(products):
                filtered_products.append(products[idx])
        
        print(f"[Stage 2] Selected {len(filtered_products)} products from {len(products)}")
        
        # If no products matched remaining terms, return top products by popularity
        if not filtered_products:
            print("[Stage 2] No content matches found - falling back to popularity sorting")
            return sorted(products, 
                         key=lambda p: (p.productView, p.rating, -p.salePrice), 
                         reverse=True)[:8]
        
        return filtered_products[:8]  # Limit to top 8
        
    except Exception as error:
        print(f"Stage 2 analysis error: {error}")
        # Fallback: return products sorted by popularity
        return sorted(products, 
                     key=lambda p: (p.productView, p.rating, -p.salePrice), 
                     reverse=True)[:8]

# Combined two-stage response generator
async def generate_two_stage_response(
    user_input: str,
    stage1_result: Dict[str, Any],
    products: List[Product],
    stage2_analysis: Dict[str, Any] = None
) -> str:
    """Generate natural language response explaining the two-stage process"""
    
    if len(products) == 0:
        return await generate_two_stage_no_results(user_input, stage1_result)
    
    processed_terms = stage1_result.get("processedTerms", {})
    used_terms = processed_terms.get("used", [])
    remaining_terms = processed_terms.get("remaining", [])
    
    top_products = products[:3]
    total_results = len(products)
    
    products_info = ""
    for i, p in enumerate(top_products):
        discount = p.price - p.salePrice
        discount_text = ""
        if discount > 0:
            discount_percent = round((discount / p.price) * 100)
            price_original_formatted = f"‡∏ø{p.price:,}"
            discount_text = f" (‡∏•‡∏î {discount_percent}% ‡∏à‡∏≤‡∏Å {price_original_formatted})"
        
        price_sale_formatted = f"‡∏ø{p.salePrice:,}"
        views_formatted = f"{p.productView:,}"
        shipping_text = '‚úÖ' if p.freeShipping else '‚ùå'
        
        products_info += f"""
{i + 1}. {p.title}
   - ‡∏£‡∏≤‡∏Ñ‡∏≤: {price_sale_formatted}{discount_text}
   - ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {p.rating}/5 ({p.totalReviews} ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß)
   - ‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏°: {views_formatted} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°
   - ‡∏™‡∏ï‡πá‡∏≠‡∏Å: {p.stockQuantity} ‡∏ä‡∏¥‡πâ‡∏ô
   - ‡∏´‡∏°‡∏ß‡∏î: {p.cateName or 'N/A'}
   - ‡∏™‡πà‡∏á‡∏ü‡∏£‡∏µ: {shipping_text}"""
    
    prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ Professional IT Sales Assistant ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏£‡∏∞‡∏ö‡∏ö Two-Stage Analysis

**USER INPUT:** "{user_input}"

**TWO-STAGE PROCESSING RESULTS:**

**Stage 1 - Basic Filtering:**
- **Used Terms:** {used_terms} 
- **MongoDB Query:** {json.dumps(stage1_result.get('query', {}), ensure_ascii=False)}
- **Reasoning:** {stage1_result.get('reasoning', 'N/A')}

**Stage 2 - Content Analysis:**
- **Remaining Terms:** {remaining_terms}
- **Products Found:** {total_results} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
- **Analysis Method:** {"Deep Content Analysis" if remaining_terms else "Popularity Sorting"}

**TOP RECOMMENDATIONS:**
{products_info}

**RESPONSE INSTRUCTIONS:**
1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏Ç‡∏≤‡∏¢‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û
2. ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ß‡πà‡∏≤‡∏ó‡∏≥‡πÑ‡∏°‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ô‡∏µ‡πâ (highlight key selling points)
3. ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß
4. ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: ‡∏£‡∏≤‡∏Ñ‡∏≤, ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡∏¢‡∏°, ‡∏™‡πà‡∏ß‡∏ô‡∏•‡∏î, ‡∏™‡∏ï‡πá‡∏≠‡∏Å
5. ‡πÉ‡∏ä‡πâ emoji ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
6. **‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ 2 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡πâ‡∏≤‡∏¢** (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏õ‡∏£‡πà‡∏á‡πÉ‡∏™)

**FORMAT:**
[‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ï‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥]

---
üîç **Search Process:**
- **Stage 1:** ‡∏Å‡∏£‡∏≠‡∏á {', '.join(used_terms) if used_terms else '‡πÑ‡∏°‡πà‡∏°‡∏µ'} ‚Üí MongoDB Query
- **Stage 2:** ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå {', '.join(remaining_terms) if remaining_terms else '‡πÑ‡∏°‡πà‡∏°‡∏µ'} ‚Üí Content Matching

‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå!
"""

    try:
        global client
        if client is None:
            client = get_openai_client()
            
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as error:
        print(f"Two-stage response generation error: {error}")
        return generate_two_stage_fallback_response(user_input, products, stage1_result)

async def generate_two_stage_no_results(user_input: str, stage1_result: Dict[str, Any]) -> str:
    """Generate no results response with two-stage explanation"""
    processed_terms = stage1_result.get("processedTerms", {})
    used_terms = processed_terms.get("used", [])
    remaining_terms = processed_terms.get("remaining", [])
    
    response = "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì üîç\n\n"
    
    response += "üí° **‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞:**\n"
    response += "‚Ä¢ ‡∏•‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì\n"
    response += "‚Ä¢ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á\n"
    response += "‚Ä¢ ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô\n"
    response += "‚Ä¢ ‡∏•‡∏≠‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á\n\n"
    
    response += "---\n"
    response += "üîç **Search Process:**\n"
    response += f"- **Stage 1:** ‡∏Å‡∏£‡∏≠‡∏á {', '.join(used_terms) if used_terms else '‡πÑ‡∏°‡πà‡∏°‡∏µ'} ‚Üí MongoDB Query\n"
    response += f"- **Stage 2:** ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå {', '.join(remaining_terms) if remaining_terms else '‡πÑ‡∏°‡πà‡∏°‡∏µ'} ‚Üí Content Matching\n"
    response += f"\n```json\n{json.dumps(stage1_result.get('query', {}), ensure_ascii=False, indent=2)}\n```"
    
    return response

def generate_two_stage_fallback_response(user_input: str, products: List[Product], stage1_result: Dict[str, Any]) -> str:
    """Fallback response generator for two-stage system"""
    if len(products) == 0:
        return f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏à‡∏≤‡∏Å query: {user_input} üîç"
    
    top_product = products[0]
    total_results = len(products)
    processed_terms = stage1_result.get("processedTerms", {})
    used_terms = processed_terms.get("used", [])
    remaining_terms = processed_terms.get("remaining", [])
    
    response = f"‡∏û‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ {total_results} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ üõçÔ∏è\n\n"
    response += f"‚≠ê **‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:** {top_product.title}\n"
    sale_price_formatted = f"‡∏ø{top_product.salePrice:,}"
    response += f"üí∞ **‡∏£‡∏≤‡∏Ñ‡∏≤:** {sale_price_formatted}"
    
    discount = top_product.price - top_product.salePrice
    if discount > 0:
        discount_percent = round((discount / top_product.price) * 100)
        original_price_formatted = f"‡∏ø{top_product.price:,}"
        response += f" (‡∏•‡∏î {discount_percent}% ‡∏à‡∏≤‡∏Å {original_price_formatted})"
    
    response += f"\nüì¶ **‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠:** {top_product.stockQuantity} ‡∏ä‡∏¥‡πâ‡∏ô"
    response += f"\n‚≠ê **‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô:** {top_product.rating}/5 ({top_product.totalReviews} ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß)"
    views_formatted = f"{top_product.productView:,}"
    response += f"\nüî• **‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏°:** {views_formatted} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡∏ä‡∏°"
    
    if top_product.freeShipping:
        response += "\nüöö **‡∏™‡πà‡∏á‡∏ü‡∏£‡∏µ**"
    
    if total_results > 1:
        response += f"\n\nüìã ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {total_results} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å!"
    
    response += "\n\n---\n"
    response += "üîç **Search Process:**\n"
    response += f"- **Stage 1:** ‡∏Å‡∏£‡∏≠‡∏á {', '.join(used_terms) if used_terms else '‡πÑ‡∏°‡πà‡∏°‡∏µ'} ‚Üí MongoDB Query\n"
    response += f"- **Stage 2:** ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå {', '.join(remaining_terms) if remaining_terms else '‡πÑ‡∏°‡πà‡∏°‡∏µ'} ‚Üí Content Matching"
    
    return response

# LLM Stage 3: Question Answerer for remaining question phrases
async def stage3_question_answerer(
    user_input: str,
    stage1_result: Dict[str, Any],
    selected_products: List[Product],
    remaining_questions: List[str]
) -> str:
    """
    Stage 3 LLM: Answer questions based on selected products
    Analyzes remaining question phrases and provides answers using product information
    """
    if not remaining_questions or len(selected_products) == 0:
        return ""
    
    print(f"[Stage 3] Answering questions: {remaining_questions}")
    
    # Prepare products info for analysis
    top_products = selected_products[:3]  # Analyze top 3 products
    products_info = ""
    for i, p in enumerate(top_products):
        products_info += f"""
Product {i + 1}: {p.title}
Price: ‡∏ø{p.salePrice:,}
Category: {p.cateName}
Description: {p.description[:300]}...
Rating: {p.rating}/5 ({p.totalReviews} reviews)
Stock: {p.stockQuantity}
"""
    
    prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ IT Product Expert ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤ IT

**USER INPUT:** "{user_input}"
**QUESTION PHRASES TO ANSWER:** {remaining_questions}

**SELECTED PRODUCTS FOR ANALYSIS:**
{products_info}

**STAGE 3 MISSION:**
1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏ô remaining_questions
2. ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö
3. ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö IT ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö
4. ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°

**QUESTION ANALYSIS GUIDELINES:**

**"‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°" / "gaming performance":**
- ‡∏î‡∏π CPU: Ryzen 5/7, Intel i5/i7 = ‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡πÑ‡∏î‡πâ‡∏î‡∏µ
- ‡∏î‡∏π GPU: RTX/GTX series = ‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡πÑ‡∏î‡πâ
- ‡∏î‡∏π RAM: 16GB+ = ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏°
- ‡∏î‡∏π‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà: Gaming Notebooks/Desktop = ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°

**"‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°" / "performance questions":**
- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡πÄ‡∏õ‡∏Ñ‡∏ï‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
- ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏£‡∏≤‡∏Ñ‡∏≤‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡πà‡∏≤
- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

**"‡∏î‡∏µ‡πÑ‡∏´‡∏°" / "quality questions":**
- ‡∏î‡∏π‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏µ‡∏ß‡∏¥‡∏ß ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏µ‡∏ß‡∏¥‡∏ß
- ‡∏î‡∏π‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏¥‡∏¢‡∏° (productView)
- ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏≠‡∏∑‡πà‡∏ô

**ANSWER FORMAT:**
‡∏ï‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå

‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ñ‡∏≤‡∏° ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤:
"""

    try:
        global client
        if client is None:
            client = get_openai_client()
            
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as error:
        print(f"Stage 3 question answering error: {error}")
        return generate_stage3_fallback_answer(remaining_questions, selected_products)

def generate_stage3_fallback_answer(questions: List[str], products: List[Product]) -> str:
    """Fallback answer generator for Stage 3"""
    if not questions or not products:
        return ""
    
    answers = []
    top_product = products[0]
    
    for question in questions:
        if "‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°" in question and "‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°" in question:
            # Gaming capability assessment
            if any(keyword in top_product.title.lower() for keyword in ['gaming', 'rtx', 'gtx', 'ryzen', 'intel']):
                answers.append(f"‚úÖ **{question}**: ‡πÉ‡∏ä‡πà ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏µ‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏° ‡∏à‡∏≤‡∏Å‡∏™‡πÄ‡∏õ‡∏Ñ‡∏ó‡∏µ‡πà‡∏î‡∏π‡πÅ‡∏•‡πâ‡∏ß‡∏ô‡πà‡∏≤‡∏à‡∏∞‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡∏°‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏î‡πâ")
            else:
                answers.append(f"‚ö†Ô∏è **{question}**: ‡∏≠‡∏≤‡∏à‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡πÄ‡∏ö‡∏≤‡πÜ ‡πÑ‡∏î‡πâ ‡πÅ‡∏ï‡πà‡πÄ‡∏Å‡∏°‡∏´‡∏ô‡∏±‡∏Å‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏î‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏£‡∏≤‡∏ü‡∏¥‡∏Å")
        elif "‡∏î‡∏µ‡πÑ‡∏´‡∏°" in question:
            # Quality assessment
            if top_product.rating >= 4:
                answers.append(f"‚≠ê **‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û**: ‡∏î‡∏µ! ‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô {top_product.rating}/5 ‡∏à‡∏≤‡∏Å {top_product.totalReviews} ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß")
            else:
                answers.append(f"üìä **‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û**: ‡∏û‡∏≠‡πÉ‡∏ä‡πâ ‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô {top_product.rating}/5 ‡∏à‡∏≤‡∏Å {top_product.totalReviews} ‡∏£‡∏µ‡∏ß‡∏¥‡∏ß")
    
    return "\n\n".join(answers)

def extract_question_phrases(phrases: List[str]) -> List[str]:
    """Extract phrases that are questions (for Stage 3)"""
    question_phrases = []
    
    for phrase in phrases:
        if is_question_phrase(phrase):
            question_phrases.append(phrase)
    
    return question_phrases

def is_question_phrase(phrase: str) -> bool:
    """Check if phrase is a question that needs Stage 3 analysis"""
    phrase_lower = phrase.lower()
    
    question_patterns = [
        r'\w+‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°',      # ‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°, ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°
        r'\w+‡∏î‡∏µ‡πÑ‡∏´‡∏°',       # ‡∏î‡∏µ‡πÑ‡∏´‡∏°
        r'‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£',     # ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
        r'‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£',        # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
        r'‡∏î‡∏µ‡∏°‡∏±‡πâ‡∏¢',         # ‡∏î‡∏µ‡∏°‡∏±‡πâ‡∏¢
        r'‡πÄ‡∏≠‡∏≤‡πÑ‡∏´‡∏°'          # ‡πÄ‡∏≠‡∏≤‡πÑ‡∏´‡∏°
    ]
    
    for pattern in question_patterns:
        if re.search(pattern, phrase_lower):
            return True
    
    return False

# Export main functions
__all__ = [
    'stage1_basic_query_builder',
    'stage2_content_analyzer', 
    'stage3_question_answerer',
    'generate_two_stage_response',
    'normalize_text_advanced',
    'get_comprehensive_category_mapping',
    'is_non_filter_phrase',
    'extract_question_phrases',
    'is_question_phrase'
]